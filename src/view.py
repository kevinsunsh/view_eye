from ultralytics import YOLO
from screen_to_world_locator import world_position_from_depth, screen_distance_to_world_distance
from typing import List, Any, Dict
import requests
import json
import re
import ast
import easyocr
import cv2
import os
import numpy as np
import tempfile
import shutil
from io import BytesIO
import base64
from PIL import Image
# EXR support removed - now using PNG depth maps
# try:
#     import OpenEXR  # type: ignore
#     import Imath  # type: ignore
# except Exception:
#     OpenEXR = None  # type: ignore
#     Imath = None  # type: ignore
from agent_memory.user_info.manager import DBManager as UserInfoManager
from agent_memory.prompt_manager.scene_iteams.manager import DBManager as SceneItemEntryManager
from agent_memory.prompt_manager.scene_info.manager import DBManager as SceneInfoManager
from agent_memory.prompt_manager.char_instance_info.manager import DBManager as CharInstanceInfoManager

# --------------- é€šç”¨å·¥å…· ---------------
def build_rts_matrix(translation: List[float], rotation_deg: List[float], scale: List[float]) -> np.ndarray:
    """
    æ„å»º 4x4 çš„æ—‹è½¬-å¹³ç§»-ç¼©æ”¾å˜æ¢çŸ©é˜µã€‚

    Args:
        translation: [tx, ty, tz]
        rotation_deg: [rx_deg, ry_deg, rz_deg] ä¾æ¬¡ä¸ºç»• Xã€Yã€Z è½´çš„æ¬§æ‹‰è§’(åº¦)ï¼Œå³æ‰‹åæ ‡ç³»ï¼ŒæŒ‰ ZÂ·YÂ·X ç»„åˆ
        scale: [sx, sy, sz]

    Returns:
        å½¢å¦‚ (4,4) çš„ numpy çŸ©é˜µï¼Œç­‰äº T @ R @ S
    """
    tx, ty, tz = float(translation[0]), float(translation[1]), float(translation[2])
    sx, sy, sz = float(scale[0]), float(scale[1]), float(scale[2])
    rx, ry, rz = np.deg2rad([rotation_deg[0], rotation_deg[1], rotation_deg[2]])

    # ç»• X è½´æ—‹è½¬
    cx, sxn = np.cos(rx), np.sin(rx)
    Rx = np.array([
        [1.0, 0.0, 0.0],
        [0.0, cx, -sxn],
        [0.0, sxn, cx]
    ], dtype=np.float64)

    # ç»• Y è½´æ—‹è½¬
    cy, syn = np.cos(ry), np.sin(ry)
    Ry = np.array([
        [cy, 0.0, syn],
        [0.0, 1.0, 0.0],
        [-syn, 0.0, cy]
    ], dtype=np.float64)

    # ç»• Z è½´æ—‹è½¬
    cz, szn = np.cos(rz), np.sin(rz)
    Rz = np.array([
        [cz, -szn, 0.0],
        [szn, cz, 0.0],
        [0.0, 0.0, 1.0]
    ], dtype=np.float64)

    # ç»„åˆæ—‹è½¬ï¼šZ Â· Y Â· Xï¼ˆå¸¸ç”¨çš„èˆªå‘-ä¿¯ä»°-æ¨ªæ»šé¡ºåºï¼‰
    R3 = Rz @ Ry @ Rx

    # åµŒå…¥åˆ° 4x4
    R4 = np.eye(4, dtype=np.float64)
    R4[:3, :3] = R3

    # ç¼©æ”¾çŸ©é˜µï¼ˆå„å‘å¼‚æ€§ï¼‰
    S4 = np.eye(4, dtype=np.float64)
    S4[0, 0] = sx
    S4[1, 1] = sy
    S4[2, 2] = sz

    # å¹³ç§»çŸ©é˜µ
    T4 = np.eye(4, dtype=np.float64)
    T4[:3, 3] = [tx, ty, tz]

    # æœ€ç»ˆå˜æ¢ï¼šå…ˆç¼©æ”¾ï¼Œå†æ—‹è½¬ï¼Œæœ€åå¹³ç§»
    M = T4 @ (R4 @ S4)
    return M
def _extract_and_parse_json_array(text: str):
    """
    ä»åŒ…å« Markdown ä»£ç å—æˆ–çº¯æ–‡æœ¬çš„å­—ç¬¦ä¸²ä¸­æå–å¹¶è§£æ JSON æ•°ç»„ã€‚
    - ä¼˜å…ˆæå– ```json ... ``` æˆ– ``` ... ``` ä»£ç å—å†…çš„å†…å®¹
    - å¤±è´¥åˆ™å°è¯•åœ¨å…¨æ–‡ä¸­å¯»æ‰¾é¦–ä¸ªä»¥ [ å¼€å§‹ã€] ç»“æŸçš„å¹³è¡¡æ•°ç»„
    - è§£ææ—¶å…ˆ json.loadsï¼Œå¤±è´¥åˆ™è¿›è¡Œå¸¸è§çº é”™ï¼ˆä¸­è‹±æ–‡å¼•å·ã€å•å¼•å·ã€å°¾éšé€—å·ï¼‰åå†å°è¯•
    è¿”å›: list æˆ–å¼•å‘å¼‚å¸¸
    """
    if not isinstance(text, str) or not text:
        raise ValueError("ç©ºå“åº”ï¼Œæ— æ³•è§£æJSON")

    content = text

    # 1) ä¼˜å…ˆæå– ```json ... ``` ä»£ç å—
    m = re.search(r"```json\s*([\s\S]*?)\s*```", content, re.IGNORECASE)
    if not m:
        # 2) å…¶æ¬¡æå–ä»»æ„ ``` ... ``` ä»£ç å—
        m = re.search(r"```\s*([\s\S]*?)\s*```", content)
    if m:
        candidate = m.group(1)
    else:
        # 3) æ— ä»£ç å—æ—¶ï¼Œå°è¯•åœ¨å…¨æ–‡ä¸­å¯»æ‰¾é¦–ä¸ªå¹³è¡¡çš„ JSON æ•°ç»„
        #    ä»ç¬¬ä¸€ä¸ª '[' å¼€å§‹å‘åæ‰«æï¼ŒåŒ¹é…åˆ°æˆå¯¹çš„ ']' ç»“æŸ
        start = content.find('[')
        if start == -1:
            raise ValueError("æœªæ‰¾åˆ°JSONæ•°ç»„èµ·å§‹æ ‡è®° [ ")
        depth = 0
        end = -1
        for i, ch in enumerate(content[start:], start=start):
            if ch == '[':
                depth += 1
            elif ch == ']':
                depth -= 1
                if depth == 0:
                    end = i
                    break
        if end == -1:
            raise ValueError("æœªæ‰¾åˆ°åŒ¹é…çš„ ] ä»¥ç»“æŸJSONæ•°ç»„")
        candidate = content[start:end+1]

    # æ ‡å‡†åŒ–å¸¸è§é—®é¢˜
    s = candidate.strip()
    # ç›´æ¥åˆ é™¤ä¸­æ–‡/æ™ºèƒ½å¼•å·å­—ç¬¦ï¼Œé¿å…è¢«å½“æˆ JSON å®šç•Œç¬¦å¯¼è‡´è§£æé”™è¯¯
    # ä¿ç•™ ASCII å¼•å· " å’Œ ' ä¸å˜
    s = s.replace('â€œ', '').replace('â€', '').replace('â€˜', '').replace('â€™', '')

    def _remove_trailing_commas(x: str) -> str:
        # ç§»é™¤å¯¹è±¡æˆ–æ•°ç»„æœ«å°¾å¤šä½™é€—å·: ,}\n æˆ– ,]\n
        x = re.sub(r",\s*}\s*", "}", x)
        x = re.sub(r",\s*]\s*", "]", x)
        return x

    # ç¬¬ä¸€è½®: ç›´æ¥å°è¯•ä¸¥æ ¼JSONè§£æ
    try:
        return json.loads(s)
    except Exception:
        pass

    # ç¬¬äºŒè½®: ç§»é™¤å°¾éšé€—å·
    s2 = _remove_trailing_commas(s)
    try:
        return json.loads(s2)
    except Exception:
        pass

    # ç¬¬ä¸‰è½®: å°†å•å¼•å·å¯¹è±¡è½¬æˆåŒå¼•å·ï¼ˆé¿å…æ›¿æ¢å­—ç¬¦ä¸²å†…éƒ¨åˆæ³•å†…å®¹ï¼Œå…ˆç²—ç•¥å¤„ç†ï¼‰
    # ä»…å½“çœ‹èµ·æ¥æ˜¯ Python é£æ ¼çš„ dict/list æ—¶æ‰å°è¯•
    looks_like_py = re.search(r"[\{\[]\s*'", s2) or re.search(r"'\s*:\s*", s2)
    s3 = s2
    if looks_like_py:
        # ç®€å•å°†é”®ä¸å­—ç¬¦ä¸²å€¼çš„å¼•å·æ›¿æ¢ä¸ºåŒå¼•å·
        # æ³¨æ„ï¼šè¿™ä¸æ˜¯å®Œç¾æ–¹æ¡ˆï¼Œä½†å¯¹å¸¸è§ LLM è¾“å‡ºè¶³å¤Ÿé²æ£’
        # å…ˆé¿å…æ›¿æ¢æ•°å­—/å¸ƒå°”/null
        # é‡‡ç”¨ä¸€ä¸ªæ¸©å’Œçš„æ›¿æ¢ï¼šä»…æ›¿æ¢åŒ…å›´åœ¨å¼•å·ä¸­çš„æˆå¯¹å•å¼•å·
        s3 = re.sub(r"'([^'\\]*(?:\\.[^'\\]*)*)'", r'"\1"', s2)
    try:
        return json.loads(s3)
    except Exception:
        pass

    # ç¬¬å››è½®: å…œåº•ä½¿ç”¨ ast.literal_eval å°è¯• Python å­—é¢é‡
    try:
        return ast.literal_eval(s3)
    except Exception as e:
        # ä»å¤±è´¥åˆ™æŠ›é”™å¹¶å¸¦ä¸Šç‰‡æ®µç”¨äºå®šä½é—®é¢˜
        snippet = s3[:500]
        raise ValueError(f"JSONè§£æå¤±è´¥: {e}. ç‰‡æ®µ: {snippet}")

# åˆå§‹åŒ–EasyOCRï¼Œæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼ˆå¯æŒ‡å®šæ¨¡å‹ç›®å½•ï¼‰
# _easyocr_model_dir = os.environ.get("EASYOCR_MODEL_DIR", "./easyocr_model")
# if _easyocr_model_dir and os.path.isdir(_easyocr_model_dir):
#     print(f"ä½¿ç”¨è‡ªå®šä¹‰ EasyOCR æ¨¡å‹ç›®å½•: {_easyocr_model_dir}")
#     reader = easyocr.Reader(['ch_sim', 'en'], model_storage_directory=_easyocr_model_dir, download_enabled=False)
# else:
#     reader = easyocr.Reader(['ch_sim', 'en'])

# OCR function disabled - EasyOCR reader not initialized
# def fast_ocr(img_input):
#     """
#     ä½¿ç”¨EasyOCRè¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œè¿”å›æ–‡å­—å†…å®¹å’Œä½ç½®ä¿¡æ¯
#     
#     Returns:
#         List[Dict]: åŒ…å«æ–‡å­—å†…å®¹å’Œè¾¹ç•Œæ¡†ä¿¡æ¯çš„åˆ—è¡¨
#     """
#     # æ”¯æŒä¼ å…¥è·¯å¾„æˆ–å†…å­˜ä¸­çš„å›¾åƒ(ndarray)
#     if isinstance(img_input, np.ndarray):
#         img = img_input
#     else:
#         img = cv2.imread(img_input)
#     original_h, original_w = img.shape[:2]
#     
#     # å¯é€‰ï¼šç¼©æ”¾å›¾åƒåˆ° 800px å®½åº¦ä»¥å†…åŠ é€Ÿ
#     scale_factor = 1.0
#     if original_w > 800:
#         scale_factor = 800 / original_w
#         img = cv2.resize(img, (800, int(800 * original_h / original_w)))
#     
#     # EasyOCRè¿”å›æ ¼å¼: [[[x1,y1], [x2,y2], [x3,y3], [x4,y4]], text, confidence]
#     result = reader.readtext(img)
#     
#     # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°OCRè¿”å›çš„æ•°æ®ç»“æ„
#     print(f"EasyOCRè¿”å›ç»“æœç±»å‹: {type(result)}")
#     print(f"EasyOCRç»“æœæ•°é‡: {len(result)}")
#     if result:
#         print(f"ç¬¬ä¸€ä¸ªç»“æœç¤ºä¾‹: {result[0]}")
#     
#     ocr_results = []
#     for i, detection in enumerate(result):
#         try:
#             # EasyOCRè¿”å›æ ¼å¼: [[[x1,y1], [x2,y2], [x3,y3], [x4,y4]], text, confidence]
#             points = detection[0]  # å››ä¸ªç‚¹çš„åæ ‡
#             text_content = detection[1]  # æ–‡å­—å†…å®¹
#             confidence = detection[2]  # ç½®ä¿¡åº¦
#             
#             print(f"å¤„ç†OCRç»“æœ {i}: æ–‡å­—='{text_content}', ç½®ä¿¡åº¦={confidence:.3f}")
#             
#             # è®¡ç®—è¾¹ç•Œæ¡†
#             x_coords = [point[0] for point in points]
#             y_coords = [point[1] for point in points]
#             x1, x2 = min(x_coords), max(x_coords)
#             y1, y2 = min(y_coords), max(y_coords)
#             
#             # å¦‚æœå›¾åƒè¢«ç¼©æ”¾ï¼Œéœ€è¦å°†åæ ‡è¿˜åŸåˆ°åŸå§‹å°ºå¯¸
#             if scale_factor != 1.0:
#                 x1 = x1 / scale_factor
#                 y1 = y1 / scale_factor
#                 x2 = x2 / scale_factor
#                 y2 = y2 / scale_factor
#             
#             ocr_results.append({
#                 'text': text_content,
#                 'confidence': confidence,
#                 'x1': x1,
#                 'y1': y1,
#                 'x2': x2,
#                 'y2': y2,
#                 'type': 'text'  # æ ‡è®°ä¸ºæ–‡å­—ç±»å‹
#             })
#         except Exception as e:
#             print(f"å¤„ç†OCRç»“æœæ—¶å‡ºé”™: {e}, detection={detection}")
#             continue
#     
#     return ocr_results

def cluster_ocr_results(ocr_results, distance_threshold=50):
    """
    å°†OCRç»“æœæŒ‰ä½ç½®èšç±»ï¼Œç›¸è¿‘ä½ç½®çš„æ–‡å­—åˆå¹¶ä¸ºä¸€ä¸ªå¯¹è±¡
    
    Args:
        ocr_results: OCRè¯†åˆ«ç»“æœåˆ—è¡¨
        distance_threshold: èšç±»è·ç¦»é˜ˆå€¼ï¼ˆåƒç´ ï¼‰
    
    Returns:
        List[Dict]: èšç±»åçš„OCRç»“æœ
    """
    if not ocr_results:
        return []
    
    clusters = []
    
    for ocr_item in ocr_results:
        # è®¡ç®—å½“å‰æ–‡å­—çš„ä¸­å¿ƒç‚¹
        center_x = (ocr_item['x1'] + ocr_item['x2']) / 2
        center_y = (ocr_item['y1'] + ocr_item['y2']) / 2
        
        # å¯»æ‰¾æœ€è¿‘çš„èšç±»
        closest_cluster = None
        min_distance = float('inf')
        
        for i, cluster in enumerate(clusters):
            # è®¡ç®—åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»
            cluster_center_x = (cluster['x1'] + cluster['x2']) / 2
            cluster_center_y = (cluster['y1'] + cluster['y2']) / 2
            
            distance = ((center_x - cluster_center_x) ** 2 + (center_y - cluster_center_y) ** 2) ** 0.5
            
            if distance < distance_threshold and distance < min_distance:
                min_distance = distance
                closest_cluster = i
        
        if closest_cluster is not None:
            # åˆå¹¶åˆ°ç°æœ‰èšç±»
            cluster = clusters[closest_cluster]
            cluster['texts'].append(ocr_item['text'])
            cluster['confidences'].append(ocr_item['confidence'])
            
            # æ›´æ–°è¾¹ç•Œæ¡†
            cluster['x1'] = min(cluster['x1'], ocr_item['x1'])
            cluster['y1'] = min(cluster['y1'], ocr_item['y1'])
            cluster['x2'] = max(cluster['x2'], ocr_item['x2'])
            cluster['y2'] = max(cluster['y2'], ocr_item['y2'])
            
            # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
            cluster['confidence'] = sum(cluster['confidences']) / len(cluster['confidences'])
            
            # æ›´æ–°åˆå¹¶åçš„æ–‡å­—å†…å®¹
            cluster['text'] = ' '.join(cluster['texts'])
        else:
            # åˆ›å»ºæ–°çš„èšç±»
            new_cluster = {
                'text': ocr_item['text'],
                'confidence': ocr_item['confidence'],
                'x1': ocr_item['x1'],
                'y1': ocr_item['y1'],
                'x2': ocr_item['x2'],
                'y2': ocr_item['y2'],
                'type': 'text',
                'texts': [ocr_item['text']],  # å­˜å‚¨æ‰€æœ‰æ–‡å­—
                'confidences': [ocr_item['confidence']]  # å­˜å‚¨æ‰€æœ‰ç½®ä¿¡åº¦
            }
            clusters.append(new_cluster)
    
    # æ¸…ç†ä¸´æ—¶å­—æ®µï¼Œåªä¿ç•™æœ€ç»ˆç»“æœéœ€è¦çš„å­—æ®µ
    for cluster in clusters:
        del cluster['texts']
        del cluster['confidences']
    
    return clusters

# å¯¼å…¥æ•°æ®åº“ç›¸å…³æ¨¡å—
from agent_memory.prompt_manager.scene_iteams.manager import DBManager as SceneItemsManager

class ChatModel():
    def __init__(self, model_name: str, model_provider: str, api_key: str, api_base: str):
        self.model_name = model_name
        self.model_provider = model_provider
        self.api_key = api_key
        self.api_base = api_base
    
    def invoke(self, messages: List[Any], extra_body: Dict[str, Any] = None) -> Any:
        """
        åŸºäºARK APIçš„åŒæ­¥èŠå¤©æ–¹æ³•ï¼Œæ›¿ä»£åŸæœ‰çš„chat_model.invoke

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            extra_body: é¢å¤–å‚æ•°ï¼ˆå¯é€‰ï¼‰

        Returns:
            èŠå¤©æ¥å£è¿”å›çš„å†…å®¹
        """
        # æ„å»ºè¯·æ±‚æ•°æ®
        payload = {
            "messages": [],
            "model": self.model_name,
            "stream": False
        }

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        for message in messages:
            if isinstance(message, dict):
                # ç›´æ¥æ˜¯å­—å…¸
                if message.get('role', None) and message.get('content', None):
                    payload["messages"].append({
                        "role": message.get('role'),
                        "content": message.get('content')
                    })
                else:
                    payload["messages"].append(message)
            else:
                # å…¼å®¹langchainçš„æ¶ˆæ¯å¯¹è±¡
                if hasattr(message, "type") and hasattr(message, "content"):
                    payload["messages"].append({
                        "role": getattr(message, "type"),
                        "content": getattr(message, "content")
                    })

        # æ·»åŠ é¢å¤–å‚æ•°
        if extra_body:
            payload.update(extra_body)

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

        try:
            response = requests.post(
                f"{self.api_base}/chat/completions",
                headers=headers,
                json=payload
            )
            if response.status_code != 200:
                raise Exception(f"ARK APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
            result = response.json()
            # å…¼å®¹ARKè¿”å›æ ¼å¼
            if "choices" in result and len(result["choices"]) > 0:
                # è¿”å›ç¬¬ä¸€ä¸ªchoiceçš„messageå†…å®¹
                return type("ChatResponse", (), {"content": result["choices"][0]["message"]["content"]})()
            return result
        except Exception as e:
            raise Exception(f"invokeè¯·æ±‚å¼‚å¸¸: {e}")

class EmbeddingModel():
    def __init__(self, model_name: str, api_key: str, api_base: str):
        self.model_name = model_name
        self.api_key = api_key
        self.api_base = api_base

    def embed(self, input_text: str) -> List[float]:
        try:
            payload = {
                "input": input_text,
                "model": self.model_name
            }
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            resp = requests.post(f"{self.api_base}/embeddings", headers=headers, json=payload)
            if resp.status_code != 200:
                print(f"åµŒå…¥è¯·æ±‚å¤±è´¥: {resp.status_code} - {resp.text}")
                return []
            data = resp.json()
            emb = ((data or {}).get("data") or [{}])[0].get("embedding", [])
            
            # ç¡®ä¿æ˜¯æµ®ç‚¹æ•°ç»„
            if isinstance(emb, list):
                try:
                    emb_array = np.array(emb, dtype=float)
                    # numpy normalize
                    emb_normalized = emb_array / np.linalg.norm(emb_array)
                    # å–å‰1024ä¸ªå…ƒç´ 
                    emb_truncated = emb_normalized[:256]
                    # è½¬æ¢ä¸ºåˆ—è¡¨
                    return emb_truncated.tolist()
                except Exception:
                    return []
            return []
        except Exception as e:
            print(f"è·å–æ–‡æœ¬åµŒå…¥å¼‚å¸¸: {e}")
            return []

# EXR decoding function removed - now using PNG depth maps
# def decode_exr_from_bytes_via_tempfile(exr_bytes: bytes) -> np.ndarray:
#     """
#     å°†å­—èŠ‚å†™å…¥ä¸´æ—¶ .exr æ–‡ä»¶åä½¿ç”¨ OpenEXR + Imath è¯»å–ã€‚
#     è¿”å› float64 çš„å•é€šé“æ·±åº¦çŸ©é˜µï¼›é€šé“ä¼˜å…ˆé¡ºåºï¼šZ > R > Y > ä»»æ„ã€‚
#     """
#     if OpenEXR is None or Imath is None:
#         raise RuntimeError("OpenEXR/Imath æœªå®‰è£…ï¼Œæ— æ³•è§£æ EXRã€‚")
#
#     with tempfile.NamedTemporaryFile(suffix=".exr", delete=False) as tmpf:
#         tmp_path = tmpf.name
#         tmpf.write(exr_bytes)
#         tmpf.flush()
#
#     try:
#         exr_file = OpenEXR.InputFile(tmp_path)
#         header = exr_file.header()
#         dw = header['dataWindow']
#         h = dw.max.y - dw.min.y + 1
#         w = dw.max.x - dw.min.x + 1
#
#         # æŒ‰ä½ ä¹‹å‰çš„ä¼˜å…ˆçº§é¡ºåºå°è¯•é€šé“
#         channels_to_try = ['Y', 'Z', 'R', 'VIEW_Z', 'DEPTH']
#         available = set(header['channels'].keys())
#         chosen = None
#         arr = None
#         for ch in channels_to_try:
#             if ch not in available:
#                 continue
#             # å…ˆå°è¯• FLOATï¼Œå†å°è¯• HALF
#             try:
#                 data_str = exr_file.channel(ch, Imath.PixelType(Imath.PixelType.FLOAT))
#                 arr = np.frombuffer(data_str, dtype=np.float32)
#             except Exception:
#                 try:
#                     data_str = exr_file.channel(ch, Imath.PixelType(Imath.PixelType.HALF))
#                     arr = np.frombuffer(data_str, dtype=np.float16).astype(np.float32)
#                 except Exception:
#                     arr = None
#             if arr is not None and arr.size == h * w:
#                 chosen = ch
#                 break
#
#         # å¦‚æœä¸Šè¿°ä¼˜å…ˆçº§éƒ½æœªæˆåŠŸï¼Œå°è¯•ä»»æ„å¯ç”¨é€šé“
#         if arr is None:
#             for ch in available:
#                 try:
#                     data_str = exr_file.channel(ch, Imath.PixelType(Imath.PixelType.FLOAT))
#                     arr = np.frombuffer(data_str, dtype=np.float32)
#                 except Exception:
#                     try:
#                         data_str = exr_file.channel(ch, Imath.PixelType(Imath.PixelType.HALF))
#                         arr = np.frombuffer(data_str, dtype=np.float16).astype(np.float32)
#                     except Exception:
#                         arr = None
#                 if arr is not None and arr.size == h * w:
#                     chosen = ch
#                     break
#
#         if arr is None:
#             raise RuntimeError(f"æœªæ‰¾åˆ°æœ‰æ•ˆæ·±åº¦é€šé“ï¼Œå°è¯•: {channels_to_try}")
#
#         arr = arr.reshape((h, w)).astype(np.float64)
#         print(f"depth.exr è¯»å–æˆåŠŸï¼Œé€šé“ {chosen}ï¼Œshape={arr.shape}")
#         return arr
#     finally:
#         try:
#             os.remove(tmp_path)
#         except Exception:
#             pass

def correct_ocr_with_vl(chat_model, image_path: str, ocr_results: List[Dict]) -> List[Dict]:
    """
    ä½¿ç”¨VLæ¨¡å‹é‡æ–°æ•´åˆOCRè¯†åˆ«ç»“æœ
    
    Args:
        chat_model: èŠå¤©æ¨¡å‹å®ä¾‹
        image_path: å›¾ç‰‡è·¯å¾„
        ocr_results: OCRè¯†åˆ«ç»“æœåˆ—è¡¨
        
    Returns:
        é‡æ–°æ•´åˆåçš„OCRç»“æœåˆ—è¡¨
    """
    # æ„å»ºOCRç»“æœçš„æè¿°
    ocr_desc = "OCRè¯†åˆ«åˆ°çš„æ–‡å­—:\n"
    for i, ocr in enumerate(ocr_results):
        ocr_desc += f"{i+1}. ä½ç½®: ({ocr['x1']:.1f}, {ocr['y1']:.1f}) åˆ° ({ocr['x2']:.1f}, {ocr['y2']:.1f}), ç½®ä¿¡åº¦: {ocr['conf']:.3f}, æ–‡å­—: '{ocr['text_content']}'\n"
    
    correction_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒè¯†åˆ«ä¸“å®¶ã€‚è¯·ä»”ç»†æŸ¥çœ‹å›¾ç‰‡ï¼Œå¹¶é‡æ–°æ•´åˆOCRæ¨¡å‹çš„è¯†åˆ«ç»“æœã€‚

{ocr_desc}

è¯·æ ¹æ®å›¾ç‰‡ä¸­çš„å®é™…å†…å®¹ï¼Œé‡æ–°æ•´åˆè¿™äº›OCRè¯†åˆ«ç»“æœï¼š
1. åˆå¹¶åº”è¯¥åœ¨ä¸€èµ·çš„æ–‡å­—ï¼ˆå¦‚å®Œæ•´çš„å¥å­ã€çŸ­è¯­ã€æ ‡è¯†ç­‰ï¼‰
2. ä¿®æ­£é”™è¯¯çš„æ–‡å­—è¯†åˆ«
3. è¿‡æ»¤æ‰æ— æ„ä¹‰çš„è¯†åˆ«ç»“æœ
4. æä¾›æ›´å‡†ç¡®çš„æ–‡å­—å†…å®¹å’Œä½ç½®

## è¾“å‡ºæ ¼å¼
è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºé‡æ–°æ•´åˆç»“æœï¼š
```json
[
  {{'index': 1, 'text': 'æ­£ç¡®çš„æ–‡å­—å†…å®¹', 'x1': è¾¹ç•Œæ¡†å·¦ä¸Šè§’X, 'y1': è¾¹ç•Œæ¡†å·¦ä¸Šè§’Y, 'x2': è¾¹ç•Œæ¡†å³ä¸‹è§’X, 'y2': è¾¹ç•Œæ¡†å³ä¸‹è§’Y, 'confidence': 'high/medium/low', 'reason': 'æ•´åˆåŸå› '}},
  {{'index': 2, 'text': 'æ­£ç¡®çš„æ–‡å­—å†…å®¹', 'x1': è¾¹ç•Œæ¡†å·¦ä¸Šè§’X, 'y1': è¾¹ç•Œæ¡†å·¦ä¸Šè§’Y, 'x2': è¾¹ç•Œæ¡†å³ä¸‹è§’X, 'y2': è¾¹ç•Œæ¡†å³ä¸‹è§’Y, 'confidence': 'high/medium/low', 'reason': 'æ•´åˆåŸå› '}}
]
```

è¯·ç¡®ä¿ï¼š
1. ä»”ç»†åˆ†æå›¾ç‰‡ä¸­çš„æ¯ä¸ªæ–‡å­—åŒºåŸŸ
2. æä¾›å‡†ç¡®çš„ä¸­æ–‡æˆ–è‹±æ–‡æ–‡å­—å†…å®¹
3. ç»™å‡ºåˆç†çš„è¾¹ç•Œæ¡†åæ ‡
4. å¦‚æœåŸè¯†åˆ«æ­£ç¡®ï¼Œä¿æŒåŸç»“æœ
5. ç»™å‡ºæ•´åˆçš„ç½®ä¿¡åº¦å’ŒåŸå› 
6. è¿‡æ»¤æ‰çš„è¯†åˆ«ç»“æœä¸è¦è¾“å‡º
"""
    
    messages = [{
        "role": "system",
        "content": correction_prompt
    }, {
        "role": "user",
        "content": [
            {
                "image_url": {"url": image_path},
                "type": "image_url"
            },
            {
                "text": "è¯·é‡æ–°æ•´åˆä¸Šè¿°OCRè¯†åˆ«ç»“æœ",
                "type": "text"
            }
        ]
    }]
    
    try:
        response = chat_model.invoke(messages, extra_body={"thinking": {"type": "disabled"}})
        print(f"VLæ¨¡å‹OCRæ•´åˆå“åº”: {response.content}")
        
        # è§£æJSONå“åº”ï¼ˆæ›´å¥å£®ï¼‰
        try:
            corrections = _extract_and_parse_json_array(response.content)
        except Exception as pe:
            print(f"VLæ¨¡å‹OCRæ•´åˆJSONè§£æå¤±è´¥: {pe}")
            return []

        # è½¬æ¢ä¸ºä¸YOLOç»“æœç›¸åŒæ ¼å¼çš„OCRç»“æœ
        ocr_results = []
        for correction in corrections:
            conf_map = {'high': 0.8, 'medium': 0.6, 'low': 0.4}
            conf_val = conf_map.get(str(correction.get('confidence', 'medium')).lower(), 0.6)
            ocr_result = {
                'index': len(ocr_results) + 1,
                'x1': correction['x1'],
                'y1': correction['y1'],
                'x2': correction['x2'],
                'y2': correction['y2'],
                'conf': conf_val,
                'class_id': -1,
                'class_name': 'text',
                'corrected_class': correction.get('text', ''),
                'desc': f"æ–‡å­—: {correction.get('text', '')}",
                'text_content': correction.get('text', ''),
                'type': 'text',
                'correction_confidence': correction.get('confidence', ''),
                'correction_reason': correction.get('reason', '')
            }
            ocr_results.append(ocr_result)

        return ocr_results
    except Exception as e:
        print(f"VLæ¨¡å‹OCRæ•´åˆå¤±è´¥: {e}")
        return []

def correct_detection_with_vl(chat_model, image_base64_str: str, detection_results: List[Dict]) -> List[Dict]:
    """
    ä½¿ç”¨VLæ¨¡å‹ä¿®æ­£YOLOæ£€æµ‹ç»“æœçš„ç±»åˆ«
    
    Args:
        chat_model: èŠå¤©æ¨¡å‹å®ä¾‹
        image_base64_str: å›¾ç‰‡Base64å­—ç¬¦ä¸²
        detection_results: YOLOæ£€æµ‹ç»“æœåˆ—è¡¨
        
    Returns:
        ä¿®æ­£åçš„æ£€æµ‹ç»“æœåˆ—è¡¨
    """
    # æ„å»ºæ£€æµ‹ç»“æœçš„æè¿°
    detection_desc = "YOLOæ£€æµ‹åˆ°çš„å¯¹è±¡:\n"
    for i, det in enumerate(detection_results):
        detection_desc += f"index: {i+1}, type: {det['class_name']}\n"
    
    correction_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒè¯†åˆ«ä¸“å®¶ã€‚è¯·ä»”ç»†æŸ¥çœ‹å›¾ç‰‡ï¼Œå¹¶ä¿®æ­£YOLOæ¨¡å‹çš„æ£€æµ‹ç»“æœã€‚

{detection_desc}

è¯·æ ¹æ®å›¾ç‰‡ä¸­çš„å®é™…å†…å®¹ï¼Œä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„å¯¹è±¡æä¾›æœ€ç¬¦åˆç”»é¢å†…å®¹çš„ã€åˆç†è€Œè¯¦ç»†çš„å¯¹è±¡æè¿°ã€‚

## é‡è¦åŸåˆ™
- æ‰¾åˆ°ç”»é¢ä¸­æœ€æ¥è¿‘è¯¥ç‰©ä½“çš„å®ä½“ï¼Œç»™å‡ºåˆç†çš„å¯¹è±¡æè¿°ã€‚
- ä¿®æ­£åº”åŸºäºè§†è§‰ç›¸ä¼¼æ€§ã€ä¸Šä¸‹æ–‡è¯­ä¹‰ã€ç‰©ä½“å¸¸è§è¯¯æ£€æ¨¡å¼ç­‰è¿›è¡Œåˆç†æ¨æ–­ã€‚

## è¾“å‡ºæ ¼å¼
è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºä¿®æ­£ç»“æœï¼š
```json
[
  {{'index': 1, type: 'type_name', 'desc': 'è¯¦ç»†çš„å¯¹è±¡æè¿°'}},
  {{'index': 2, type: 'type_name', 'desc': 'è¯¦ç»†çš„å¯¹è±¡æè¿°'}}
]
```

## è¾“å‡ºè¦æ±‚
1. ä»”ç»†åˆ†æå›¾ç‰‡ä¸­çš„æ¯ä¸ªæ£€æµ‹åŒºåŸŸ
2. ä¸ºæ¯ä¸ªå¯¹è±¡æä¾›è¯¦ç»†çš„å¤–è§‚æè¿°ï¼ˆé¢œè‰²ã€å½¢çŠ¶ã€æè´¨ã€ä½ç½®ç­‰ï¼‰
"""
    
    messages = [{
        "role": "system",
        "content": correction_prompt
    }, {
        "role": "user",
        "content": [
            {
                "image_url": {"url": f"data:image/jpeg;base64,{image_base64_str}"},
                "type": "image_url"
            },
            {
                "text": "è¯·ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„å¯¹è±¡æä¾›æœ€ç¬¦åˆç”»é¢å†…å®¹çš„ã€åˆç†è€Œè¯¦ç»†çš„å¯¹è±¡æè¿°",
                "type": "text"
            }
        ]
    }]

    # messages = [{
    #     "role": "user",
    #     "content": [
    #         {
    #             "image_url": {"url": image_path},
    #             "type": "image_url"
    #         },
    #         {
    #             "text": "å›¾é‡Œæœ‰ä»€ä¹ˆ",
    #             "type": "text"
    #         }
    #     ]
    # }]
    
    try:
        response = chat_model.invoke(messages, extra_body={"thinking": {"type": "disabled"}})
        print(f"VLæ¨¡å‹ä¿®æ­£å“åº”: {response.content}")
        
        # è§£æJSONå“åº”ï¼ˆæ›´å¥å£®ï¼‰
        try:
            corrections = _extract_and_parse_json_array(response.content)
            return corrections
        except Exception as pe:
            print(f"VLæ¨¡å‹ä¿®æ­£JSONè§£æå¤±è´¥: {pe}")
            return []
    except Exception as e:
        print(f"VLæ¨¡å‹ä¿®æ­£å¤±è´¥: {e}")
        return []

def handle_position(user_id:str, chat_id:str):
    chat_model = ChatModel(
        model_name=os.environ.get("VLM_MODEL_NAME", "doubao-seed-1-6-vision-250815"),
        api_key=os.environ.get("VLM_API_KEY", "dc7e10e7-1095-40ae-a172-3a7d16fc1e61"),
        api_base=os.environ.get("VLM_API_BASE", "https://ark.cn-beijing.volces.com/api/v3"),
        model_provider="openai"
    )
    embedding_model = EmbeddingModel(
        model_name=os.environ.get("EMB_MODEL_NAME", "doubao-embedding-large-text-250515"),
        api_key=os.environ.get("VLM_API_KEY", "dc7e10e7-1095-40ae-a172-3a7d16fc1e61"),
        api_base=os.environ.get("VLM_API_BASE", "https://ark.cn-beijing.volces.com/api/v3"),
    )
    # desc_vec = embedding_model.embed("æµ·æŠ¥")
    # user_info = UserInfoManager().get_user_info_by_user_id(user_id)
    # scene_id = user_info.current_scene_id
    # items = SceneItemsManager().search_items_by_description_vector(scene_id, desc_vec, top_k=10)
    # print(f"desc_vec: {items}")
    look_url = f"https://aura-view-eye.tos-cn-beijing.volces.com/assets/{user_id}/{chat_id}/view_data/look.jpg"
    cam_url = f"https://aura-view-eye.tos-cn-beijing.volces.com/assets/{user_id}/{chat_id}/view_data/cam.json"
    depth_url = f"https://aura-view-eye.tos-cn-beijing.volces.com/assets/{user_id}/{chat_id}/view_data/depth.png"
    view_eye_data = os.environ.get("VIEW_EYE_DATA")
    data_is_local = False
    if view_eye_data:
        look_url = f"{view_eye_data}{user_id}/{chat_id}/view_data/look.jpg"
        cam_url = f"{view_eye_data}{user_id}/{chat_id}/view_data/cam.json"
        depth_url = f"{view_eye_data}{user_id}/{chat_id}/view_data/depth.png"
        data_is_local = True
    # ç›´æ¥ä¸‹è½½ä¸ºå†…å­˜æ•°æ®ï¼ˆä¸è½åœ°ä¸´æ—¶æ–‡ä»¶ï¼‰
    if data_is_local == False:
        try:
            print(f"æ­£åœ¨ä¸‹è½½ look.jpg: {look_url}")
            resp_look = requests.get(look_url, timeout=30)
            resp_look.raise_for_status()
            look_bytes = np.frombuffer(resp_look.content, dtype=np.uint8)
            look_img = cv2.imdecode(look_bytes, cv2.IMREAD_COLOR)
            if look_img is None:
                raise RuntimeError("look.jpg è§£æå¤±è´¥")
            print("look.jpg ä¸‹è½½å¹¶è§£æå®Œæˆ")
            print(f"æ­£åœ¨ä¸‹è½½ cam.json: {cam_url}")
            resp_cam = requests.get(cam_url, timeout=30)
            resp_cam.raise_for_status()
            cam_data = json.loads(resp_cam.content.decode('utf-8'))
            print("cam.json ä¸‹è½½å¹¶è§£æå®Œæˆ")
            print(f"æ­£åœ¨ä¸‹è½½ depth.png: {depth_url}")
            resp_depth = requests.get(depth_url, timeout=30)
            resp_depth.raise_for_status()
            depth_content = resp_depth.content
            if depth_content is None or len(depth_content) == 0:
                raise RuntimeError("depth.png å†…å®¹ä¸ºç©º")
            print("depth.png ä¸‹è½½å®Œæˆï¼ˆå†…å­˜ï¼‰")
        except Exception as e:
            raise Exception(f"ä¸‹è½½èµ„æºå¤±è´¥: {e}")
    else:
        try:
            print("æ­£åœ¨è¯»å– local file")
            look_bytes = np.fromfile(look_url, dtype=np.uint8)
            look_img = cv2.imdecode(look_bytes, cv2.IMREAD_COLOR)
            cam_data = json.load(open(cam_url, 'r'))
            with open(depth_url, 'rb') as f:
                depth_content = f.read()
            if depth_content is None or len(depth_content) == 0:
                raise RuntimeError("depth.png å†…å®¹ä¸ºç©º(æœ¬åœ°)")
        except Exception as e:
            raise Exception(f"ä¸‹è½½èµ„æºå¤±è´¥: {e}")
    
    yolo_model_dir = os.environ.get("YOLO_MODEL_PATH")
    if yolo_model_dir:
        # model_path = f"{yolo_model_dir}/yoloe-11l-seg-pf.pt"
        model_path = f"{yolo_model_dir}yolo11l.pt"
    else:
        model_path = "./yoloe_model/yolo11l.pt"
    
    # æ™ºèƒ½åˆ¤æ–­æ¨¡å‹æ ¼å¼ï¼šä¼˜å…ˆä½¿ç”¨ONNXæ ¼å¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨PTæ ¼å¼
    onnx_path = model_path.replace(".pt", ".onnx")
    if os.path.exists(onnx_path):
        print(f"ä½¿ç”¨ONNXæ¨¡å‹: {onnx_path}")
        model = YOLO(onnx_path)
    elif os.path.exists(model_path):
        print(f"ä½¿ç”¨PTæ¨¡å‹: {model_path}")
        model = YOLO(model_path)
        model.export(format="onnx")
        print(f"è½¬æ¢ä¸ºONNXæ¨¡å‹: {onnx_path}")
        model = YOLO(onnx_path)
    else:
        raise Exception(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path} æˆ– {onnx_path}")
    # å¤„ç†å®Œæˆåæ¸…ç†ä¸´æ—¶ç›®å½•ï¼ˆå‡½æ•°çº§åˆ«ï¼‰
    # æ³¨æ„ï¼šä¸‹è½½å¼‚å¸¸æ—¶å·²æ¸…ç†å¹¶æŠ›å‡ºå¼‚å¸¸ï¼›æ­£å¸¸æµç¨‹åœ¨å‡½æ•°æœ«å°¾æ¸…ç†
    results = model.predict(look_img)
    result = results[0]
    # result.save(f"look_{user_id}_{chat_id}.jpg")
    # Get the annotated image (plot)
    annotated_img = results[0].plot()  # returns a BGR numpy array
    # Convert BGR to RGB (PIL expects RGB)
    annotated_img_rgb = annotated_img[..., ::-1]
    # Convert to PIL Image
    pil_img = Image.fromarray(annotated_img_rgb)
    # Save to BytesIO buffer as PNG (or JPEG)
    buffer = BytesIO()
    pil_img.save(buffer, format="JPEG")
    img_bytes = buffer.getvalue()

    # Encode to Base64
    base64_str = base64.b64encode(img_bytes).decode('utf-8')
    print("=== YOLOæ£€æµ‹ç»“æœè§£æ ===")
    print(f"æ£€æµ‹åˆ°çš„å¯¹è±¡æ•°é‡: {len(results[0].boxes) if results[0].boxes is not None else 0}")
    
    # ä½¿ç”¨EasyOCRè¯†åˆ«æ–‡å­—
    # print("\n=== OCRæ–‡å­—è¯†åˆ« ===")
    # ocr_results = fast_ocr(look_img)
    # print(f"è¯†åˆ«åˆ°çš„åŸå§‹æ–‡å­—æ•°é‡: {len(ocr_results)}")
    
    # å¯¹OCRç»“æœè¿›è¡Œèšç±»
    # print("\n=== OCRç»“æœèšç±» ===")
    # clustered_ocr = cluster_ocr_results(ocr_results, distance_threshold=50)
    # print(f"èšç±»åçš„æ–‡å­—å¯¹è±¡æ•°é‡: {len(clustered_ocr)}")
    
    # for i, ocr_item in enumerate(clustered_ocr):
    #     print(f"  æ–‡å­—å¯¹è±¡ {i+1}: '{ocr_item['text']}' (ç½®ä¿¡åº¦: {ocr_item['confidence']:.3f})")
    #     print(f"    ä½ç½®: ({ocr_item['x1']:.1f}, {ocr_item['y1']:.1f}) åˆ° ({ocr_item['x2']:.1f}, {ocr_item['y2']:.1f})")
    
    # è§£ææ£€æµ‹æ¡†ä¿¡æ¯å¹¶å‡†å¤‡VLä¿®æ­£
    detection_results = []
    if result.boxes is not None:
        print("\n=== åŸå§‹YOLOæ£€æµ‹ç»“æœ ===")
        for i, box in enumerate(result.boxes):
            # è·å–åæ ‡
            xyxy = box.xyxy[0].cpu().numpy()  # [x1, y1, x2, y2]
            conf = box.conf[0].cpu().numpy()  # ç½®ä¿¡åº¦
            if conf < 0.5:
                continue
            cls = int(box.cls[0].cpu().numpy())  # ç±»åˆ«ID
            
            class_name = result.names.get(cls, 'Unknown') if hasattr(result, 'names') and result.names else 'Unknown'
            class_name = class_name.replace(' ', '_')
            detection_info = {
                'index': i + 1,
                'x1': float(xyxy[0]),
                'y1': float(xyxy[1]),
                'x2': float(xyxy[2]),
                'y2': float(xyxy[3]),
                'conf': float(conf),
                'class_id': cls,
                'class_name': class_name,
                'type': 'object'  # æ ‡è®°ä¸ºç‰©ä½“ç±»å‹
            }
            detection_results.append(detection_info)
            
            print(f"  å¯¹è±¡ {i+1}:")
            print(f"    åæ ‡: x1={xyxy[0]:.1f}, y1={xyxy[1]:.1f}, x2={xyxy[2]:.1f}, y2={xyxy[3]:.1f}")
            print(f"    ç½®ä¿¡åº¦: {conf:.3f}")
            print(f"    ç±»åˆ«ID: {cls}")
            print(f"    ç±»åˆ«åç§°: {class_name}")
    
    # å°†èšç±»åçš„OCRç»“æœæ·»åŠ åˆ°æ£€æµ‹ç»“æœä¸­
    # print("\n=== åˆå¹¶èšç±»åçš„OCRæ–‡å­—æ£€æµ‹ç»“æœ ===")
    # for i, ocr_item in enumerate(clustered_ocr):
    #     ocr_detection = {
    #         'index': len(detection_results) + i + 1,
    #         'x1': ocr_item['x1'],
    #         'y1': ocr_item['y1'],
    #         'x2': ocr_item['x2'],
    #         'y2': ocr_item['y2'],
    #         'conf': ocr_item['confidence'],
    #         'class_id': -1,  # OCRæ–‡å­—ä½¿ç”¨ç‰¹æ®ŠID
    #         'class_name': 'text',
    #         'text_content': ocr_item['text'],  # å­˜å‚¨èšç±»åçš„æ–‡å­—å†…å®¹
    #         'type': 'text'  # æ ‡è®°ä¸ºæ–‡å­—ç±»å‹
    #     }
    #     detection_results.append(ocr_detection)
    #     print(f"  æ–‡å­—å¯¹è±¡ {i+1}: '{ocr_item['text']}' (ç½®ä¿¡åº¦: {ocr_item['confidence']:.3f})")
    
    print(f"\n=== æ€»æ£€æµ‹ç»“æœç»Ÿè®¡ ===")
    print(f"YOLOæ£€æµ‹å¯¹è±¡æ•°é‡: {len([d for d in detection_results if d['type'] == 'object'])}")
    # print(f"OCRè¯†åˆ«æ–‡å­—æ•°é‡: {len([d for d in detection_results if d['type'] == 'text'])}")
    print(f"æ€»æ£€æµ‹æ•°é‡: {len(detection_results)}")
    
    # åˆ†ç¦»YOLOå’ŒOCRç»“æœ
    # yolo_results = [d for d in detection_results if d['type'] == 'object']
    # ocr_results = [d for d in detection_results if d['type'] == 'text']
    
    # è¯»å–å›¾åƒå°ºå¯¸ç”¨äºåƒç´ ->NDCè½¬æ¢
    try:
        img_h, img_w = (look_img.shape[0], look_img.shape[1]) if look_img is not None else (1080, 1920)
    except Exception:
        img_h, img_w = (1080, 1920)
    inv_mvp = None
    depth_linear = None
    # near_plane = 0.1
    # far_plane = 100.0
    
    try:
        print(f"æ­£åœ¨è§£æ cam.jsonï¼ˆå†…å­˜ï¼‰")
        print(f"cam.json ä¸‹è½½æˆåŠŸï¼ŒåŒ…å« {len(cam_data)} ä¸ªé”®")
        # ä¼˜å…ˆè¯»å–åˆ†ç¦»çš„ proj/view çŸ©é˜µ
        proj = cam_data.get('projection_matrix')
        view_m = cam_data.get('view_matrix')
        scene_name = cam_data.get('scene_name')
        # # è¿‘è¿œè£å‰ªé¢ï¼ˆè‹¥æä¾›ï¼‰
        # if isinstance(cam_data.get('near'), (int, float)):
        #     near_plane = float(cam_data['near'])
        # if isinstance(cam_data.get('far'), (int, float)):
        #     far_plane = float(cam_data['far'])
        if isinstance(proj, list) and len(proj) == 16 and isinstance(view_m, list) and len(view_m) == 16:
            proj_matrix = np.array([proj[i*4:(i+1)*4] for i in range(4)], dtype=np.float64).transpose()
            view_matrix = np.array([view_m[i*4:(i+1)*4] for i in range(4)], dtype=np.float64).transpose()
            print("å·²è§£æåˆ†ç¦»çš„ proj/view çŸ©é˜µ (4x4)")
            try:
                inv_proj_matrix = np.linalg.inv(proj_matrix)
                inv_view_matrix = np.linalg.inv(view_matrix)
                camera_position = inv_view_matrix[:3, 3]
                cam_fwd_h = inv_view_matrix @ np.array([0.0, 0.0, 1.0, 0.0], dtype=np.float64)
                camera_forward = cam_fwd_h[:3]
                n = np.linalg.norm(camera_forward)
                if n > 0:
                    camera_forward = camera_forward / n
            except Exception as e:
                print(f"è®¡ç®— inv_proj/inv_view æˆ–ç›¸æœºå‚æ•°å¤±è´¥: {e}")
    except requests.exceptions.RequestException as e:
        print(f"ä¸‹è½½ cam.json å¤±è´¥: {e}")
    except json.JSONDecodeError as e:
        print(f"è§£æ cam.json å¤±è´¥: {e}")
    except Exception as e:
        print(f"å¤„ç† cam.json æ—¶å‡ºé”™: {e}")
    # è¯»å– PNG æ·±åº¦å›¾ï¼ˆå†…å­˜ï¼‰
    try:
        print(f"æ­£åœ¨è§£æ depth.pngï¼ˆå†…å­˜ï¼‰")
        # ä½¿ç”¨ OpenCV è§£ç  PNG æ·±åº¦å›¾ï¼ˆä¿æŒä½æ·±/é€šé“ï¼‰
        depth_buf = np.frombuffer(depth_content, dtype=np.uint8)
        depth_img = cv2.imdecode(depth_buf, cv2.IMREAD_UNCHANGED)
        if depth_img is None:
            raise RuntimeError("depth.png è§£ç å¤±è´¥")
        # æœŸæœ›æ ¼å¼ï¼š32ä½è‰²ï¼Œfloat åˆ†4å­—èŠ‚å‹å…¥ RGBA
        # OpenCV è§£ç è¿”å›é€šé“é¡ºåºä¸º BGRAï¼ˆè‹¥æœ‰4é€šé“ï¼‰
        if depth_img.ndim == 3 and depth_img.shape[2] == 4:
            # æå–é€šé“ï¼ˆB,G,R,Aï¼‰
            b = depth_img[:, :, 0]
            g = depth_img[:, :, 1]
            r = depth_img[:, :, 2]
            a = depth_img[:, :, 3]
            # è¿˜åŸä¸ºå°ç«¯åº float32 å­—èŠ‚åºåˆ— [R,G,B,A]
            rgba_bytes = np.stack([r, g, b, a], axis=-1).astype(np.uint8)
            flat_bytes = rgba_bytes.reshape(-1, 4)
            # é€šè¿‡è§†å›¾è½¬æ¢ä¸º float32ï¼Œå† reshape å›åŸå°ºå¯¸
            depth_f32 = flat_bytes.view(np.float32).reshape(depth_img.shape[0], depth_img.shape[1])
            depth_linear = depth_f32.astype(np.float64)
        else:
            raise RuntimeError("depth.png é€šé“æ•°ä¸ä¸º4ï¼Œæ— æ³•æŒ‰ RGBA æ‰“åŒ…è§„åˆ™è§£æ")
        print(f"depth.png è§£ææˆåŠŸï¼ˆRGBA-packed float32ï¼‰ï¼Œshape={depth_linear.shape}, å€¼èŒƒå›´=[{np.nanmin(depth_linear):.6f}, {np.nanmax(depth_linear):.6f}]")
    except Exception as e:
        print(f"è¯»å– depth.png å¤±è´¥: {e}")
    # è§†å£çŸ©å½¢
    view_rect = (0, 0, img_w, img_h)
    
    # æ‰¹é‡å­˜å‚¨ï¼šä¸¤é˜¶æ®µ
    # é˜¶æ®µ1ï¼šè®¡ç®—ä¸–ç•Œåæ ‡å¹¶åŸºäºè·ç¦»å¤ç”¨å·²æœ‰ Itemï¼Œä»…æ›´æ–°åæ ‡ç­‰åŠ¨æ€å­—æ®µ
    # é˜¶æ®µ2ï¼šå¯¹æœªåŒ¹é…çš„æ–°é¡¹ä½¿ç”¨ VLM æ›´æ­£ï¼Œå†æ‰¹é‡å†™å…¥
    if detection_results:
        try:
            print("\n=== ç¬¬ä¸€é˜¶æ®µï¼šåŒ¹é…å·²æœ‰é¡¹å¹¶æ‰¹é‡æ›´æ–°åæ ‡ ===")
            db_manager = SceneItemsManager()
            
            # è·å–åœºæ™¯IDï¼ˆç”¨äºè·ç¦»åŒ¹é…ä¸å†™åº“ï¼‰
            scene_info = SceneInfoManager().get_scene_info_by_scene_name(scene_name)
            scene_id = scene_info.scene_id
            char_instance_info = CharInstanceInfoManager().get_char_instance_info_by_user_and_chat_id(user_id, chat_id)
            if char_instance_info is None:
                CharInstanceInfoManager().upsert_char_instance_info(user_id, chat_id, current_scene_id=scene_id)
            else:
                if char_instance_info.current_scene_id != scene_id:
                    CharInstanceInfoManager().upsert_char_instance_info(user_id, chat_id, current_scene_id=scene_id, view_matrix=char_instance_info.view_matrix, projection_matrix=char_instance_info.projection_matrix, char_status=char_instance_info.char_status)
            # å…ˆä¸ºå…¨éƒ¨æ£€æµ‹è®¡ç®—ä¸–ç•Œåæ ‡
            matched_nodes = []
            unmatched_indices = []
            world_positions = []
            world_sizes = []
            base_names: List[str] = []
            for idx, detection in enumerate(detection_results):
                # åˆå§‹åç§°/æè¿°ï¼ˆä»…æœªåŒ¹é…ç”¨äºäºŒé˜¶æ®µçº æ­£å‰çš„å ä½ï¼‰
                if detection['type'] == 'object':
                    base_name = detection.get('class_name', 'Unknown')
                    base_desc = detection.get('class_name', 'æ— æè¿°')
                else:
                    base_name = 'text'
                    base_desc = detection.get('text_content', 'text')
                
                # è®¡ç®—åƒç´ ä¸­å¿ƒç‚¹
                cx_px = (detection['x2'] - detection['x1']) / 2 + detection['x1']
                cy_px = (detection['y2'] - detection['y1']) / 2 + detection['y1']
                translation_vec = [0.0, 0.0, 0.0]
                world_size = 0.3
                if inv_proj_matrix is not None and inv_view_matrix is not None and depth_linear is not None and camera_position is not None and camera_forward is not None:
                    try:
                        # è·å–æ£€æµ‹åŒºåŸŸçš„è¾¹ç•Œ
                        x1 = int(np.clip(np.floor(detection['x1']), 0, img_w - 1))
                        y1 = int(np.clip(np.floor(detection['y1']), 0, img_h - 1))
                        x2 = int(np.clip(np.floor(detection['x2']), 0, img_w - 1))
                        y2 = int(np.clip(np.floor(detection['y2']), 0, img_h - 1))
                        
                        # ç¡®ä¿åŒºåŸŸæœ‰æ•ˆ
                        if x2 > x1 and y2 > y1:
                            # è·å–æ£€æµ‹åŒºåŸŸå†…çš„æ·±åº¦å€¼å¹¶æ‰¾åˆ°ä¸­å€¼
                            detection_depth_region = depth_linear[y1:y2, x1:x2]
                            # è¿‡æ»¤æ‰æ— æ•ˆçš„æ·±åº¦å€¼ï¼ˆé€šå¸¸ä¸º0æˆ–è´Ÿæ•°ï¼‰
                            valid_depths = detection_depth_region[detection_depth_region > 0]
                            if len(valid_depths) > 0:
                                depth_value = float(np.median(valid_depths))
                            else:
                                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ·±åº¦å€¼ï¼Œä½¿ç”¨ä¸­å¿ƒç‚¹
                                u0 = int(np.clip(np.floor(cx_px), 0, img_w - 1))
                                v0 = int(np.clip(np.floor(cy_px), 0, img_h - 1))
                                depth_value = float(depth_linear[v0, u0])
                        else:
                            # å¦‚æœåŒºåŸŸæ— æ•ˆï¼Œä½¿ç”¨ä¸­å¿ƒç‚¹
                            u0 = int(np.clip(np.floor(cx_px), 0, img_w - 1))
                            v0 = int(np.clip(np.floor(cy_px), 0, img_h - 1))
                            depth_value = float(depth_linear[v0, u0])
                        world = world_position_from_depth(
                            np.array([cx_px, cy_px], dtype=np.float64),
                            view_rect,
                            inv_view_matrix,
                            inv_proj_matrix,
                            camera_position,
                            camera_forward,
                            depth_value,
                        )
                        world_size = screen_distance_to_world_distance(
                            np.array([detection['x2'], detection['y2']], dtype=np.float64),
                            np.array([detection['x1'], detection['y1']], dtype=np.float64),
                            depth_value,
                            depth_value,
                            view_rect,
                            inv_view_matrix,
                            inv_proj_matrix,
                            camera_position,
                            camera_forward,
                        )
                        translation_vec = [float(world[0]), float(world[1]), float(world[2])]
                    except Exception as e:
                        print(f"å±å¹•->ä¸–ç•Œåæ ‡è½¬æ¢å¤±è´¥(idx={idx}): {e}")
                world_positions.append(translation_vec)
                world_sizes.append(world_size)
                base_names.append(base_name)
            # try:
            #     if 'proj_matrix' in locals() and 'view_matrix' in locals():
            #         view_proj_matrix = proj_matrix @ view_matrix
            #         existing_items = db_manager.get_scene_items_by_distance(camera_position, 100000, scene_id, ["Nova", "Zoe", "Eva", "Novaè€", "nova", "MHC_Talker"])
            #         existing_items = db_manager.get_scene_items_in_frustum_by_view_proj(view_proj_matrix, existing_items)
            # except Exception as _:
            #     pass
            item_type_groups = {}
            for item_type in base_names:
                if item_type not in item_type_groups:
                    item_type_groups[item_type] = db_manager.get_scene_items_by_type(scene_id, item_type)
            # view_proj_matrix = view_matrix @ proj_matrix
            # existing_items = db_manager.query_items_in_frustum_by_vp(scene_id, view_proj_matrix)
            # AABB ç›¸äº¤åŒ¹é…ï¼ˆåŒç±»å‹/å‰ç¼€ï¼‰ï¼Œç›¸äº¤åˆ™è®¤ä¸ºåŒä¸€IDï¼›å¦‚å¤šé¡¹ç›¸äº¤ï¼Œå–äº¤å ä½“ç§¯æœ€å¤§çš„
            # def nearest_existing_id(pos: list[float], base_name: str, size: float) -> str:
            #     half = max(0.01, float(size) / 2.0)
            #     new_min = [pos[0] - half, pos[1] - half, pos[2] - half]
            #     new_max = [pos[0] + half, pos[1] + half, pos[2] + half]
            #     def overlap1d(a_min: float, a_max: float, b_min: float, b_max: float) -> float:
            #         return max(0.0, min(a_max, b_max) - max(a_min, b_min))
            #     def overlap_volume(a_min, a_max, b_min, b_max) -> float:
            #         ox = overlap1d(a_min[0], a_max[0], b_min[0], b_max[0])
            #         oy = overlap1d(a_min[1], a_max[1], b_min[1], b_max[1])
            #         oz = overlap1d(a_min[2], a_max[2], b_min[2], b_max[2])
            #         return ox * oy * oz
            #     best_item = None
            #     best_vol = 0.0
            #     candidates = item_type_groups.get(base_name, [])
            #     for item in candidates:
            #         it = item.to_dict()
            #         ex_min = [
            #             float(it.get('world_bb_x', it.get('world_pos_x', 0.0))),
            #             float(it.get('world_bb_y', it.get('world_pos_y', 0.0))),
            #             float(it.get('world_bb_z', it.get('world_pos_z', 0.0)))
            #         ]
            #         ex_max = [
            #             ex_min[0] + float(it.get('world_bb_w', 0.0)),
            #             ex_min[1] + float(it.get('world_bb_h', 0.0)),
            #             ex_min[2] + float(it.get('world_bb_d', 0.0))
            #         ]
            #         vol = overlap_volume(new_min, new_max, ex_min, ex_max)
            #         if vol > best_vol:
            #             best_vol = vol
            #             best_item = it
            #     return best_item if best_vol > 0.0 else None
            # ç¬¬ä¸€é˜¶æ®µï¼šåŒ¹é…å·²çŸ¥å¯¹è±¡å¹¶æ›´æ–°ä½ç½®/bboxï¼Œè¯†åˆ«æ–°å¯¹è±¡
            print("\n=== ç¬¬ä¸€é˜¶æ®µï¼šåŒ¹é…å·²çŸ¥å¯¹è±¡å¹¶è¯†åˆ«æ–°å¯¹è±¡ ===")
            
            def is_point_in_bbox(point, bbox_min, bbox_max):
                """æ£€æŸ¥ç‚¹æ˜¯å¦åœ¨åŒ…å›´ç›’å†…"""
                return (bbox_min[0] <= point[0] <= bbox_max[0] and
                        bbox_min[1] <= point[1] <= bbox_max[1] and
                        bbox_min[2] <= point[2] <= bbox_max[2])
            
            def find_matching_item(pos, item_type, existing_items):
                """åœ¨ç°æœ‰ç‰©å“ä¸­æŸ¥æ‰¾åŒ¹é…çš„ç‰©å“ï¼ˆåŒç±»å‹ä¸”åœ¨bboxå†…ï¼‰"""
                for item in existing_items:
                    item_dict = item.to_dict() if hasattr(item, 'to_dict') else item
                    if item_dict.get('item_type') == item_type:
                        # è®¡ç®—ç°æœ‰ç‰©å“çš„bbox
                        ex_bb_x = float(item_dict.get('world_bb_x', item_dict.get('world_pos_x', 0.0)))
                        ex_bb_y = float(item_dict.get('world_bb_y', item_dict.get('world_pos_y', 0.0)))
                        ex_bb_z = float(item_dict.get('world_bb_z', item_dict.get('world_pos_z', 0.0)))
                        ex_bb_w = float(item_dict.get('world_bb_w', 0.0))
                        ex_bb_h = float(item_dict.get('world_bb_h', 0.0))
                        ex_bb_d = float(item_dict.get('world_bb_d', 0.0))
                        
                        bbox_min = [ex_bb_x, ex_bb_y, ex_bb_z]
                        bbox_max = [ex_bb_x + ex_bb_w, ex_bb_y + ex_bb_h, ex_bb_z + ex_bb_d]
                        
                        if is_point_in_bbox(pos, bbox_min, bbox_max):
                            return item_dict
                return None
            
            batch_update_nodes = []  # ç”¨äºæ›´æ–°å·²çŸ¥å¯¹è±¡çš„èŠ‚ç‚¹
            new_items = []  # æ–°å¯¹è±¡åˆ—è¡¨
            
            for idx, detection in enumerate(detection_results):
                pos = world_positions[idx]
                size = world_sizes[idx]
                if pos is None:
                    continue
                
                item_type = detection.get('class_name', 'Unknown') if detection['type'] == 'object' else 'text'
                
                # åœ¨ç›¸åŒç±»å‹çš„ç°æœ‰ç‰©å“ä¸­æŸ¥æ‰¾åŒ¹é…é¡¹
                existing_items = item_type_groups.get(item_type, [])
                matched_item = find_matching_item(pos, item_type, existing_items)
                
                if matched_item:
                    # æ‰¾åˆ°åŒ¹é…çš„å·²çŸ¥å¯¹è±¡ï¼Œåªæ›´æ–°ä½ç½®å’Œbbox
                    print(f"ğŸ”„ æ›´æ–°å·²çŸ¥å¯¹è±¡: {matched_item.get('item_name', item_type)}")
                    
                    half = max(0.01, float(size) / 2.0)
                    bb_min = [pos[0] - half, pos[1] - half, pos[2] - half]
                    bb_max = [pos[0] + half, pos[1] + half, pos[2] + half]
                    
                    node = {
                        'item_type': item_type,
                        'item_name': matched_item.get('item_name', item_type),
                        'description': matched_item.get('description', item_type),
                        'description_vector': matched_item.get('description_vector', []),
                        'translation': pos,
                        'extras': {
                            'tags': [matched_item.get('item_id', f"{item_type}_0")],
                            'boundingBox': {
                                'min': bb_min,
                                'max': bb_max
                            },
                            'actions': matched_item.get('actions', {}),
                            'skills': matched_item.get('skills', {}),
                            'detection_type': detection['type'],
                            'confidence': detection['conf'],
                            'class_id': detection.get('class_id', -1),
                            'original_class': detection.get('class_name', 'Unknown')
                        }
                    }
                    batch_update_nodes.append(node)
                else:
                    # æœªæ‰¾åˆ°åŒ¹é…é¡¹ï¼Œè®¤ä¸ºæ˜¯æ–°å¯¹è±¡
                    print(f"ğŸ†• å‘ç°æ–°å¯¹è±¡: {item_type}")
                    new_items.append({
                        'detection': detection,
                        'pos': pos,
                        'size': size,
                        'idx': idx
                    })
            
            # æ‰¹é‡æ›´æ–°å·²çŸ¥å¯¹è±¡
            if batch_update_nodes:
                _ = db_manager.upsert_scene_items_batch(scene_id, batch_update_nodes)
                print(f"âœ… å·²æ›´æ–° {len(batch_update_nodes)} ä¸ªå·²çŸ¥å¯¹è±¡")
            
            # ç¬¬äºŒé˜¶æ®µï¼šå¯¹æ–°å¯¹è±¡è¿›è¡Œ VLM ä¼˜åŒ–
            if new_items:
                print(f"\n=== ç¬¬äºŒé˜¶æ®µï¼šå¯¹ {len(new_items)} ä¸ªæ–°å¯¹è±¡è¿›è¡Œ VLM ä¼˜åŒ– ===")
                
                # æå–æ£€æµ‹å¯¹è±¡ç”¨äº VLM å¤„ç†
                new_detections = [item['detection'] for item in new_items if item['detection']['type'] == 'object']
                
                if new_detections:
                    print(f"ğŸ” å¼€å§‹å¯¹ {len(new_detections)} ä¸ªæ–°å¯¹è±¡è¿›è¡Œ VLM ä¼˜åŒ–...")
                    obj_corr = correct_detection_with_vl(chat_model, base64_str, new_detections)
                    
                    # åˆ›å»ºä¼˜åŒ–åçš„èŠ‚ç‚¹æ•°æ®
                    batch_optimized_nodes = []
                    for j, item_info in enumerate(new_items):
                        if item_info['detection']['type'] != 'object':
                            continue
                            
                        det = item_info['detection']
                        pos = item_info['pos']
                        size = item_info['size']
                        
                        # è·å– VLM ä¼˜åŒ–ç»“æœ
                        corrected_name = obj_corr[j].get('corrected_class', det.get('class_name', 'Unknown')) if obj_corr and j < len(obj_corr) else det.get('class_name', 'Unknown')
                        corrected_desc = obj_corr[j].get('desc', corrected_name) if obj_corr and j < len(obj_corr) else corrected_name
                        desc_vec = embedding_model.embed(corrected_desc) if corrected_desc else []
                        
                        # è®¡ç®—åŒ…å›´ç›’
                        half = max(0.01, float(size) / 2.0)
                        bb_min = [pos[0] - half, pos[1] - half, pos[2] - half]
                        bb_max = [pos[0] + half, pos[1] + half, pos[2] + half]
                        
                        item_type = det.get('class_name', 'Unknown')
                        # ä¸ºæ–°å¯¹è±¡ç”Ÿæˆæ–°çš„ç´¢å¼•
                        current_count = len([item for item in item_type_groups.get(item_type, []) if item])
                        item_id = f"{item_type}_{current_count}"
                        
                        node = {
                            'item_type': item_type,
                            'item_name': corrected_name,
                            'description': corrected_desc,
                            'description_vector': desc_vec,
                            'translation': pos,
                            'extras': {
                                'tags': [item_id],
                                'boundingBox': {
                                    'min': bb_min,
                                    'max': bb_max
                                },
                                'actions': {},
                                'skills': {},
                                'detection_type': det['type'],
                                'confidence': det['conf'],
                                'class_id': det.get('class_id', -1),
                                'original_class': det.get('class_name', 'Unknown')
                            }
                        }
                        batch_optimized_nodes.append(node)
                        item_type_groups[item_type].append(node)
                    
                    # æ‰¹é‡æ’å…¥æ–°å¯¹è±¡
                    if batch_optimized_nodes:
                        _ = db_manager.upsert_scene_items_batch(scene_id, batch_optimized_nodes)
                        print(f"âœ… å·²åˆ›å»º {len(batch_optimized_nodes)} ä¸ªæ–°å¯¹è±¡å¹¶å®Œæˆ VLM ä¼˜åŒ–")
            else:
                print("\n=== ç¬¬äºŒé˜¶æ®µï¼šæœªå‘ç°æ–°å¯¹è±¡ï¼Œè·³è¿‡ VLM ä¼˜åŒ– ===")
                
        except Exception as e:
            print(f"âŒ æ‰¹é‡å­˜å‚¨å¯¹è±¡åˆ°æ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    # æ— éœ€æ¸…ç†ä¸´æ—¶ç›®å½•ï¼ˆå·²å…¨ç¨‹ä½¿ç”¨å†…å­˜æ•°æ®ï¼‰

